Answers for Evgeni (Eugene) Borissov for CMPT732.    


1. What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

Answer: The size of the input files weren't all the same; one of the files was much larger than the rest.  Repartitioning repartitioned the input files to be of approximately equal size, which spread the computing load equally across the executors. 



2. The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

Answer: The number of input files is large, and no single file is very big. Therefore, the cost of repartitioning is greater than the benefits.



3. How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

Answer: Have the input files evenly partitioned, so that repartitioning isn't required.



4. When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

Answer: Running it on the lab computers, the range of partitions between 4 and approximately 100 is "good". I didn't see any benefit in increasing the number of partitions above 100.



5. How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation? 

Answer: Based on the run times below, the overhead is approximately 31 seconds (1m15s - 44s).
   Spark Standard Python with 10 partitions: 2m5s
   Spark Standard Python single-threaded: 6m7s
   Spark PyPy single-threaded: 1m15s
   Non-Spark PyPy single-threaded: 44s
   Non-Spark C single-threaded: 27s

